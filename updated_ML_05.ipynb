{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a169d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Attribute\n",
      "\n",
      "Linear Regression (Single Attribute):\n",
      "Train MSE: 1.3618586756930053\n",
      "Test MSE: 1.3885595634666783\n",
      "Train RMSE: 1.1669870075082265\n",
      "Test RMSE: 1.1783715727505812\n",
      "Train MAPE: 142324572671023.0\n",
      "Test MAPE: 60703281590984.914\n",
      "Train R²: 0.0052754335314767475\n",
      "Test R²: -0.00881950367199047\n",
      "\n",
      "Classification (Single Attribute):\n",
      "First 5 Predictions (Train): [0 0 0 0 0]\n",
      "First 5 Predictions (Test): [0 0 0 0 0]\n",
      "\n",
      "Classification Report (Train):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       681\n",
      "           1       0.00      0.00      0.00       219\n",
      "\n",
      "    accuracy                           0.76       900\n",
      "   macro avg       0.38      0.50      0.43       900\n",
      "weighted avg       0.57      0.76      0.65       900\n",
      "\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       181\n",
      "           1       0.00      0.00      0.00        45\n",
      "\n",
      "    accuracy                           0.80       226\n",
      "   macro avg       0.40      0.50      0.44       226\n",
      "weighted avg       0.64      0.80      0.71       226\n",
      "\n",
      "\n",
      "Multiple Attributes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression (Multiple Attributes):\n",
      "Train MSE: 0.3445174376113472\n",
      "Test MSE: 1.8349981047834891\n",
      "Train RMSE: 0.5869560780938785\n",
      "Test RMSE: 1.3546210188770471\n",
      "Train MAPE: 24193397001284.453\n",
      "Test MAPE: 12819173092524.688\n",
      "Train R²: 0.7483586477176825\n",
      "Test R²: -0.3331670646414773\n",
      "\n",
      "Classification (Multiple Attributes):\n",
      "First 5 Predictions (Train): [0 0 0 0 0]\n",
      "First 5 Predictions (Test): [0 1 0 0 1]\n",
      "\n",
      "Classification Report (Train):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91       681\n",
      "           1       0.85      0.48      0.61       219\n",
      "\n",
      "    accuracy                           0.85       900\n",
      "   macro avg       0.85      0.73      0.76       900\n",
      "weighted avg       0.85      0.85      0.84       900\n",
      "\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       181\n",
      "           1       0.61      0.49      0.54        45\n",
      "\n",
      "    accuracy                           0.84       226\n",
      "   macro avg       0.75      0.71      0.72       226\n",
      "weighted avg       0.83      0.84      0.83       226\n",
      "\n",
      "\n",
      "Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarun\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering Metrics:\n",
      "Silhouette Score: 0.09723250955473711\n",
      "Calinski-Harabasz Score: 128.59142025741957\n",
      "Davies-Bouldin Index: 2.59612760923113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to perform regression and classification with a single feature\n",
    "def a1(file):\n",
    "    df = pd.read_excel(file)\n",
    "    X = df[['embed_0']]\n",
    "    y = df['output']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_train_pred_reg = reg.predict(X_train)\n",
    "    y_test_pred_reg = reg.predict(X_test)\n",
    "\n",
    "    # Convert target variable to binary class based on threshold\n",
    "    y_train_class = y_train.apply(lambda x: 1 if x > 4 else 0)\n",
    "    y_test_class = y_test.apply(lambda x: 1 if x > 4 else 0)\n",
    "    \n",
    "    clf = LogisticRegression().fit(X_train, y_train_class)\n",
    "    y_train_pred_clf = clf.predict(X_train)\n",
    "    y_test_pred_clf = clf.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        \"reg_model\": reg,\n",
    "        \"X_train\": X_train, \"X_test\": X_test,\n",
    "        \"y_train\": y_train, \"y_test\": y_test,\n",
    "        \"y_train_pred_reg\": y_train_pred_reg, \"y_test_pred_reg\": y_test_pred_reg,\n",
    "        \"clf_model\": clf,\n",
    "        \"y_train_class\": y_train_class, \"y_test_class\": y_test_class,\n",
    "        \"y_train_pred_clf\": y_train_pred_clf, \"y_test_pred_clf\": y_test_pred_clf\n",
    "    }\n",
    "    \n",
    "# Function to calculate regression metrics\n",
    "def a2(results):\n",
    "    mse_train = mean_squared_error(results[\"y_train\"], results[\"y_train_pred_reg\"])\n",
    "    mse_test = mean_squared_error(results[\"y_test\"], results[\"y_test_pred_reg\"])\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    mape_train = mean_absolute_percentage_error(results[\"y_train\"], results[\"y_train_pred_reg\"])\n",
    "    mape_test = mean_absolute_percentage_error(results[\"y_test\"], results[\"y_test_pred_reg\"])\n",
    "    r2_train = r2_score(results[\"y_train\"], results[\"y_train_pred_reg\"])\n",
    "    r2_test = r2_score(results[\"y_test\"], results[\"y_test_pred_reg\"])\n",
    "\n",
    "    return {\n",
    "        \"mse_train\": mse_train, \"mse_test\": mse_test,\n",
    "        \"rmse_train\": rmse_train, \"rmse_test\": rmse_test,\n",
    "        \"mape_train\": mape_train, \"mape_test\": mape_test,\n",
    "        \"r2_train\": r2_train, \"r2_test\": r2_test\n",
    "    }\n",
    "    \n",
    "# Function to perform regression and classification with multiple features\n",
    "def a3(file):\n",
    "    df = pd.read_excel(file)\n",
    "    X = df.drop(columns=['output'])\n",
    "    y = df['output']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_train_pred_reg = reg.predict(X_train)\n",
    "    y_test_pred_reg = reg.predict(X_test)\n",
    "\n",
    "    # Convert target variable to binary class based on threshold\n",
    "    y_train_class = y_train.apply(lambda x: 1 if x > 4 else 0)\n",
    "    y_test_class = y_test.apply(lambda x: 1 if x > 4 else 0)\n",
    "    \n",
    "    clf = LogisticRegression().fit(X_train, y_train_class)\n",
    "    y_train_pred_clf = clf.predict(X_train)\n",
    "    y_test_pred_clf = clf.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        \"reg_model\": reg,\n",
    "        \"X_train\": X_train, \"X_test\": X_test,\n",
    "        \"y_train\": y_train, \"y_test\": y_test,\n",
    "        \"y_train_pred_reg\": y_train_pred_reg, \"y_test_pred_reg\": y_test_pred_reg,\n",
    "        \"clf_model\": clf,\n",
    "        \"y_train_class\": y_train_class, \"y_test_class\": y_test_class,\n",
    "        \"y_train_pred_clf\": y_train_pred_clf, \"y_test_pred_clf\": y_test_pred_clf\n",
    "    }\n",
    "    \n",
    "    metrics = a2(results)\n",
    "    \n",
    "    return results, metrics\n",
    "    \n",
    "# Function to perform KMeans clustering\n",
    "def a4(file):\n",
    "    df = pd.read_excel(file)\n",
    "    X = df.drop(columns=['output'])\n",
    "    \n",
    "    # Fit KMeans clustering with 2 clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "    \n",
    "    return kmeans.labels_, kmeans.cluster_centers_, X\n",
    "\n",
    "# Function to calculate clustering metrics\n",
    "def a5(X, labels):\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    ch_score = calinski_harabasz_score(X, labels)\n",
    "    db_index = davies_bouldin_score(X, labels)\n",
    "    \n",
    "    return {\n",
    "        \"silhouette_score\": silhouette,\n",
    "        \"calinski_harabasz_score\": ch_score,\n",
    "        \"davies_bouldin_index\": db_index\n",
    "    }\n",
    "\n",
    "# Function to evaluate clustering performance for a range of k values\n",
    "def a6(file, max_k=10):\n",
    "    df = pd.read_excel(file)\n",
    "    X = df.drop(columns=['output'])\n",
    "\n",
    "    silhouette_scores = []\n",
    "    ch_scores = []\n",
    "    db_indices = []\n",
    "    \n",
    "    k_values = range(2, max_k + 1)\n",
    "    \n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "        labels = kmeans.labels_\n",
    "        \n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "        ch_scores.append(calinski_harabasz_score(X, labels))\n",
    "        db_indices.append(davies_bouldin_score(X, labels))\n",
    "    \n",
    "    plt.figure(figsize=(5, 4)) \n",
    "    plt.plot(k_values, silhouette_scores, label=\"Silhouette Score\", marker='o')\n",
    "    plt.plot(k_values, ch_scores, label=\"Calinski-Harabasz Score\", marker='s')\n",
    "    plt.plot(k_values, db_indices, label=\"Davies-Bouldin Index\", marker='d')\n",
    "    \n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Clustering Scores vs Number of Clusters\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot the elbow method to find the optimal number of clusters\n",
    "def a7(file):\n",
    "    df = pd.read_excel(file)\n",
    "    X = df.drop(columns=['output'])\n",
    "\n",
    "    distortions = []\n",
    "    \n",
    "    for k in range(2, 20):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(range(2, 20), distortions, marker='o')\n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"Distortion (Inertia)\")\n",
    "    plt.title(\"Elbow Method for Optimal k\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    file = 'C:\\\\Users\\\\tarun\\\\Lab Codes\\\\training_mathbert.xlsx'\n",
    "    \n",
    "    print(\"Single Attribute\")\n",
    "    results = a1(file)\n",
    "    metrics = a2(results)\n",
    "\n",
    "    # Print regression metrics\n",
    "    print(\"\\nLinear Regression (Single Attribute):\")\n",
    "    print(f\"Train MSE: {metrics['mse_train']}\")\n",
    "    print(f\"Test MSE: {metrics['mse_test']}\")\n",
    "    print(f\"Train RMSE: {metrics['rmse_train']}\")\n",
    "    print(f\"Test RMSE: {metrics['rmse_test']}\")\n",
    "    print(f\"Train MAPE: {metrics['mape_train']}\")\n",
    "    print(f\"Test MAPE: {metrics['mape_test']}\")\n",
    "    print(f\"Train R²: {metrics['r2_train']}\")\n",
    "    print(f\"Test R²: {metrics['r2_test']}\")\n",
    "\n",
    "    # Classification results\n",
    "    print(\"\\nClassification (Single Attribute):\")\n",
    "    print(f\"First 5 Predictions (Train): {results['y_train_pred_clf'][:5]}\")\n",
    "    print(f\"First 5 Predictions (Test): {results['y_test_pred_clf'][:5]}\")\n",
    "    print(\"\\nClassification Report (Train):\")\n",
    "    print(classification_report(results[\"y_train_class\"], results[\"y_train_pred_clf\"]))\n",
    "    print(\"\\nClassification Report (Test):\")\n",
    "    print(classification_report(results[\"y_test_class\"], results[\"y_test_pred_clf\"]))\n",
    "\n",
    "    # With multiple attributes\n",
    "    print(\"\\nMultiple Attributes\")\n",
    "    results, metrics = a3(file)\n",
    "    \n",
    "    print(\"\\nLinear Regression (Multiple Attributes):\")\n",
    "    print(f\"Train MSE: {metrics['mse_train']}\")\n",
    "    print(f\"Test MSE: {metrics['mse_test']}\")\n",
    "    print(f\"Train RMSE: {metrics['rmse_train']}\")\n",
    "    print(f\"Test RMSE: {metrics['rmse_test']}\")\n",
    "    print(f\"Train MAPE: {metrics['mape_train']}\")\n",
    "    print(f\"Test MAPE: {metrics['mape_test']}\")\n",
    "    print(f\"Train R²: {metrics['r2_train']}\")\n",
    "    print(f\"Test R²: {metrics['r2_test']}\")\n",
    "    \n",
    "    print(\"\\nClassification (Multiple Attributes):\")\n",
    "    print(f\"First 5 Predictions (Train): {results['y_train_pred_clf'][:5]}\")\n",
    "    print(f\"First 5 Predictions (Test): {results['y_test_pred_clf'][:5]}\")\n",
    "    print(\"\\nClassification Report (Train):\")\n",
    "    print(classification_report(results[\"y_train_class\"], results[\"y_train_pred_clf\"]))\n",
    "    print(\"\\nClassification Report (Test):\")\n",
    "    print(classification_report(results[\"y_test_class\"], results[\"y_test_pred_clf\"]))\n",
    "\n",
    "    # Clustering results\n",
    "    print(\"\\nClustering\")\n",
    "    labels, centers, X = a4(file)\n",
    "    \n",
    "    print(\"\\nClustering Metrics:\")\n",
    "    metrics = a5(X, labels)\n",
    "    print(f\"Silhouette Score: {metrics['silhouette_score']}\")\n",
    "    print(f\"Calinski-Harabasz Score: {metrics['calinski_harabasz_score']}\")\n",
    "    print(f\"Davies-Bouldin Index: {metrics['davies_bouldin_index']}\")\n",
    "    \n",
    "    # Plot Elbow Method and Clustering Scores\n",
    "    a7(file)\n",
    "    a6(file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
